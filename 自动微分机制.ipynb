{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 自动微分机制\n",
        "torch中的核心概念是torch.tensor, 即张量。\n",
        "\n",
        "张量的重要性质是 .requires_grad, 如果设置它的属性 .requires_grad 为 True，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用 .backward()，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到.grad属性。  对于自己定义的张量，默认.requires_grad=False, 通过torch神经网络模型生成的张量（包括网络的可训练参数）默认为True。\n",
        "\n",
        "除此之外，还有两个性质：.grad 和 .grad_fn\n",
        "\n",
        "(1).grad是一个指向该张量创建过程的函数的引用，它指示了这个张量是通过某种操作（例如加法、矩阵乘法等）生成的。它保存着该张量的计算历史，因此可以追踪它在计算图中的前向传播过程。如果这个张量是由用户直接创建的（例如通过 torch.tensor() 创建的常量张量），它的 grad_fn 为 None。\n",
        "\n",
        "(2).grad是一个张量的属性，用来存储该张量的梯度。梯度是通过反向传播计算得到的，它代表了该张量相对于某个标量值（通常是损失函数）的导数。在反向传播（backward()）后，grad 属性会存储计算得到的梯度值。通常，你会通过 tensor.grad 来访问这个梯度。如果一个张量没有被标记为需要梯度（即 requires_grad=False），或者没有进行反向传播，grad 就会是 None。\n",
        "\n",
        "当然，可以阻止计算被追踪。除了将.requries_grad设置为False外，可以通过将代码块包装在 with torch.no_grad(): 中，来阻止 autograd 跟踪设置了.requires_grad=True的张量的历史记录。"
      ],
      "metadata": {
        "id": "z8eKq6kY9D7P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iwRzaLIPmuWE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 1))"
      ],
      "metadata": {
        "id": "lUgVy_-qnUS2"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in net.named_parameters():\n",
        "    print(f\"Layer: {name}, Shape: {param.shape}, {param.requires_grad}, {param.grad_fn}, {param.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_slv7Xi4K7P",
        "outputId": "ae4ec455-1ee8-414e-8d18-e3918939a584"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: 0.weight, Shape: torch.Size([256, 20]), True, None, None\n",
            "Layer: 0.bias, Shape: torch.Size([256]), True, None, None\n",
            "Layer: 2.weight, Shape: torch.Size([1, 256]), True, None, None\n",
            "Layer: 2.bias, Shape: torch.Size([1]), True, None, None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成随机输入（batch_size=10, 输入特征维度=20）\n",
        "input_data = torch.randn(10, 20)  # 10个样本，每个样本20维\n",
        "\n",
        "# 前向传播\n",
        "output = net(input_data)  # 计算输出\n",
        "\n",
        "# 定义损失函数（假设是回归任务，使用均方误差损失）\n",
        "loss_fn = nn.MSELoss()\n",
        "target = torch.randn(10, 1)  # 随机生成目标值\n",
        "loss = loss_fn(output, target)  # 计算损失\n",
        "\n",
        "# 反向传播\n",
        "loss.backward()  # 计算梯度"
      ],
      "metadata": {
        "id": "FTICTYEBBPEu"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 反向传播后.grad_fn和.grad属性不再是None\n",
        "for name, param in net.named_parameters():\n",
        "    print(f\"Layer: {name}, Shape: {param.shape}, {param.requires_grad}, {param.grad_fn}, {param.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFQXEc4MBPPQ",
        "outputId": "44d27b17-faf6-4af8-e789-1a61f2a0934d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: 0.weight, Shape: torch.Size([256, 20]), True, None, tensor([[-0.0063, -0.0110,  0.0008,  ...,  0.0019, -0.0017,  0.0031],\n",
            "        [-0.0037, -0.0250,  0.0034,  ..., -0.0187,  0.0047, -0.0020],\n",
            "        [ 0.0147,  0.0230,  0.0092,  ..., -0.0045, -0.0030, -0.0060],\n",
            "        ...,\n",
            "        [-0.0016, -0.0247, -0.0006,  ..., -0.0378, -0.0037,  0.0063],\n",
            "        [ 0.0052,  0.0164, -0.0107,  ...,  0.0380,  0.0040, -0.0056],\n",
            "        [-0.0024, -0.0184, -0.0044,  ...,  0.0008,  0.0002, -0.0038]])\n",
            "Layer: 0.bias, Shape: torch.Size([256]), True, None, tensor([ 2.6050e-03,  1.7577e-02,  2.0176e-03,  0.0000e+00, -4.8657e-03,\n",
            "         4.3071e-03,  1.5231e-02, -3.4564e-02, -1.9429e-03,  1.9044e-02,\n",
            "        -5.0433e-05, -1.4930e-02,  2.1513e-02,  1.9944e-03, -1.9099e-02,\n",
            "        -9.1793e-07,  1.0618e-03,  3.2822e-02,  2.7492e-03, -3.5686e-02,\n",
            "        -2.7442e-02, -3.6546e-04, -1.0519e-02,  7.6228e-03, -2.9492e-02,\n",
            "         2.2992e-03, -5.5533e-03,  5.0763e-03, -3.6265e-03, -5.0055e-03,\n",
            "        -2.7256e-02, -1.3284e-02,  2.7576e-03,  1.7577e-03, -1.4634e-02,\n",
            "         4.8632e-03,  9.3601e-03,  5.0994e-04,  2.1358e-02, -1.5520e-02,\n",
            "         1.3255e-03, -1.0195e-03, -2.9759e-03, -3.1840e-02,  7.6345e-05,\n",
            "         8.9753e-03,  1.5877e-02,  6.3562e-03,  3.3981e-04, -1.0986e-02,\n",
            "        -3.1906e-03, -5.5267e-03, -1.4387e-02, -5.0329e-03, -6.6610e-05,\n",
            "         4.7233e-02, -2.7296e-02,  4.1818e-03,  2.2365e-02, -2.4252e-04,\n",
            "         9.5251e-03,  2.8368e-03,  2.3366e-02,  1.9312e-04, -2.0249e-02,\n",
            "         2.9089e-03,  7.8807e-03, -3.7707e-02,  9.4585e-03,  1.4623e-03,\n",
            "        -2.4543e-02, -3.9595e-02,  3.1810e-03,  2.0346e-02, -8.5030e-03,\n",
            "        -2.2084e-02, -1.4787e-02,  1.1667e-02, -8.8975e-03, -5.3373e-03,\n",
            "         1.0557e-02,  2.6671e-02,  2.4962e-03, -1.9301e-03,  3.2036e-04,\n",
            "         4.0632e-03,  9.1550e-04, -6.6017e-03,  4.2502e-03, -7.5847e-04,\n",
            "         7.5687e-03,  4.0575e-04,  9.5140e-05,  3.7963e-03, -3.0387e-05,\n",
            "        -2.9740e-03, -3.0269e-03, -2.5259e-02,  5.6778e-03,  2.8664e-03,\n",
            "        -9.0736e-03, -1.0729e-02,  8.3250e-03,  2.3736e-02,  4.0843e-02,\n",
            "        -2.6697e-03, -6.9351e-03, -4.0075e-02,  4.3890e-02, -2.9584e-02,\n",
            "         1.7668e-02, -3.9228e-02, -3.8052e-03, -1.5387e-02, -6.6529e-03,\n",
            "         3.6511e-02, -1.4363e-02, -3.7903e-03, -3.5022e-03,  9.8324e-03,\n",
            "         2.5089e-02, -2.6218e-03, -5.4574e-03,  9.9962e-03, -9.6733e-03,\n",
            "         5.2331e-03, -1.6990e-02,  2.5116e-02,  5.2660e-03, -3.6743e-03,\n",
            "         1.5167e-02,  1.6924e-02, -9.6998e-04, -6.3408e-03, -1.4377e-02,\n",
            "        -4.5184e-03,  1.2631e-02,  8.1300e-03, -5.0909e-03, -1.3716e-02,\n",
            "         3.1128e-02,  1.4768e-02, -2.3076e-02,  1.6822e-02,  5.1723e-03,\n",
            "        -6.1420e-03,  1.2067e-02,  3.6698e-03, -2.7743e-04, -2.7268e-02,\n",
            "        -4.1445e-05,  2.6554e-02, -5.8086e-04,  1.5537e-02,  6.5856e-03,\n",
            "         3.8223e-02,  2.6122e-02, -2.1151e-02,  1.7376e-02,  6.6056e-04,\n",
            "        -2.6853e-03,  6.2685e-03, -1.5419e-02,  1.1020e-02,  9.1926e-03,\n",
            "         5.3028e-03,  1.2224e-02, -3.3925e-02,  1.0658e-02,  2.4714e-02,\n",
            "         2.3284e-02, -7.1494e-03,  1.4951e-03,  8.9886e-03, -1.5414e-02,\n",
            "        -5.8767e-03,  3.7398e-03,  1.9269e-03, -7.1067e-03,  6.0197e-03,\n",
            "         2.3391e-03,  6.4744e-04, -7.2246e-04, -4.8365e-03, -4.2220e-03,\n",
            "        -1.3710e-03,  4.1478e-04,  9.6922e-04,  3.3984e-02,  7.3190e-03,\n",
            "         7.3485e-03, -1.5470e-02,  9.5068e-04, -1.1778e-03, -1.9497e-02,\n",
            "        -1.0796e-02, -1.8897e-02,  1.3364e-02,  6.0508e-03,  3.9609e-03,\n",
            "         2.5042e-04, -1.7177e-02, -1.3813e-02, -1.7344e-02,  7.7641e-03,\n",
            "        -2.3620e-02, -4.3802e-03, -1.2378e-03,  7.2687e-03,  2.7218e-03,\n",
            "        -2.2638e-03, -1.6983e-04, -6.5455e-04,  8.5237e-04, -2.2070e-02,\n",
            "         9.1004e-03,  4.5925e-03, -2.9250e-02, -5.0118e-03,  1.1132e-03,\n",
            "        -1.5400e-03,  1.8804e-02, -4.3513e-03, -2.7494e-03, -3.1644e-03,\n",
            "         1.0046e-03,  7.8999e-03,  3.8994e-02, -1.1869e-03,  4.4157e-02,\n",
            "        -1.2154e-02,  1.2324e-02, -3.8916e-04,  1.4155e-02,  1.5638e-02,\n",
            "         6.1023e-04,  1.3058e-03,  6.2161e-03,  1.5947e-02,  3.1592e-03,\n",
            "         5.0422e-03, -3.3809e-02, -5.1823e-03, -1.8927e-03, -1.3760e-02,\n",
            "        -3.1914e-04,  1.9789e-02, -1.6934e-02, -7.0211e-03, -2.0818e-03,\n",
            "        -1.1415e-03, -6.7357e-05,  1.2855e-03,  4.8500e-02, -5.4574e-02,\n",
            "        -3.2780e-03])\n",
            "Layer: 2.weight, Shape: torch.Size([1, 256]), True, None, tensor([[ 0.1444,  0.0615, -0.0611,  0.0000, -0.0078,  0.8374,  0.1465,  0.1712,\n",
            "         -0.0959,  0.2501, -0.1409,  0.2993,  0.4102,  0.3128,  0.5490, -0.2445,\n",
            "          0.1455,  0.1752,  0.3727,  0.2180,  0.3255, -0.2207, -0.0547,  0.0958,\n",
            "          0.2074,  0.1887, -0.0222,  0.2541,  0.1003,  0.3188,  0.5441,  0.1989,\n",
            "          0.0237,  0.0160,  0.0787, -0.0445, -0.1764, -0.1187,  0.0290,  0.5376,\n",
            "          0.0342, -0.0565, -0.2151,  0.4529,  0.0250,  0.2911,  0.2699,  0.0553,\n",
            "          0.0941, -0.1421,  0.4025,  0.0092,  0.2012,  0.0487,  0.1364,  0.0892,\n",
            "          0.0741, -0.4855,  0.4867,  0.2420,  0.2019,  0.0673,  0.2978, -0.0205,\n",
            "          0.3691,  0.0767, -0.2472,  0.1499,  0.1640, -0.0405,  0.0804,  0.2452,\n",
            "          0.0131,  0.5532,  0.0384,  0.0550,  0.0433, -0.4200,  0.0445,  0.3075,\n",
            "          0.1498,  0.0981,  0.0023,  0.0229, -0.0092,  0.5198,  0.0103,  0.1013,\n",
            "         -0.1924,  0.1232,  0.1833, -0.0025, -0.0074,  0.1540,  0.1305,  0.0648,\n",
            "          0.3092,  0.2602,  0.4997,  0.2653,  0.2160,  0.1637,  0.1560,  0.0702,\n",
            "          0.1299,  0.0710, -0.1494,  0.3920,  0.1536,  0.4058,  0.5945,  0.3760,\n",
            "          0.0435,  0.0349, -0.1235,  0.3280,  0.1485,  0.0507, -0.1315,  0.0991,\n",
            "          0.0879,  0.0884,  0.2783,  0.0656,  0.0306,  0.0370, -0.3461,  0.2937,\n",
            "          0.3261,  0.1814,  0.3891,  0.0452,  0.0451, -0.1871, -0.2704,  0.5391,\n",
            "          0.2624,  0.1278,  0.2318,  0.3383,  0.2988,  0.3679,  0.2413,  0.4211,\n",
            "          0.2482,  0.1864,  0.4033,  0.0955,  0.0759,  0.6141, -0.1433,  0.3199,\n",
            "          0.1308, -0.1412,  0.1638,  0.7493,  0.3111,  0.1756,  0.4350,  0.0268,\n",
            "          0.3482,  0.1587,  0.1371, -0.0363,  0.2944, -0.0650,  0.1240,  0.3960,\n",
            "          0.1641, -0.4431,  0.1524,  0.0653,  0.1216,  0.1018,  0.0166, -0.0069,\n",
            "         -0.1408,  0.3167,  0.2448, -0.2513, -0.3389,  0.0029,  0.1141, -0.0028,\n",
            "         -0.1655,  0.4239,  0.0198, -0.0431,  0.2091, -0.0532,  0.0964,  0.0313,\n",
            "         -0.0029,  0.3662,  0.2821,  0.3335,  0.0027,  0.1224, -0.3194,  0.2411,\n",
            "         -0.4689,  0.1150,  0.1086,  0.3018,  0.2094,  0.4283, -0.0257, -0.1390,\n",
            "          0.2256,  0.0200,  0.2091,  0.3097,  0.1674, -0.0548,  0.4930,  0.0353,\n",
            "          0.3066,  0.4254,  0.1539, -0.0427,  0.0236, -0.0418, -0.3446,  0.4568,\n",
            "          0.1224,  0.0151,  0.1884,  0.2734,  0.1468,  0.4189,  0.3306,  0.0899,\n",
            "         -0.1492, -0.1642,  0.4382, -0.1462, -0.6243,  0.1547,  0.3276, -0.1282,\n",
            "          0.1347,  0.3119, -0.0194, -0.3207,  0.1815, -0.2530,  0.3933,  0.3480,\n",
            "          0.1923, -0.0053,  0.1102, -0.1945,  0.1498,  0.2400,  0.2314, -0.1468]])\n",
            "Layer: 2.bias, Shape: torch.Size([1]), True, None, tensor([0.6704])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSUUWBG2BTYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HRcX8O_17vxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.tensor([2.0], requires_grad=True)"
      ],
      "metadata": {
        "id": "Y_yryfPB8NhL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A.requires_grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoPsUhwF0ajb",
        "outputId": "4bcce3cd-724b-4739-8695-3cb615d6edf3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.grad_fn"
      ],
      "metadata": {
        "id": "1_5tUdFf8WxZ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B=A**2"
      ],
      "metadata": {
        "id": "7frwVqEozODW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B.requires_grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlCFyc53zT1i",
        "outputId": "a3629f3c-b37c-4873-efe7-e9093e1748e1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B.grad_fn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrrRrWR1ze27",
        "outputId": "71a14bed-95f5-4dbc-8732-26855c2fd7c5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PowBackward0 at 0x7f08077b3520>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AcHZdiMa8Szt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qsukpklN9HJc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}